#!/usr/bin/env python3
import logging
import pandas as pd
from pathlib import Path
from joblib import dump

import optuna
from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, precision_score, confusion_matrix
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.base import BaseEstimator, TransformerMixin
from imblearn.pipeline import Pipeline as ImbPipeline

SEED = 32
OUTPUT_DIR = Path("output/mlp_fs_90d_final")
DATA_PATH = Path('datasets/train_test_202511_80')
MODEL_FILENAME = "mlp_90d_final.joblib"
N_TRIALS = 300

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler(OUTPUT_DIR / 'training.log'), logging.StreamHandler()])


class PostProcessingImputer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.lab_prefixes_ = ['df_bili', 'df_crea', 'df_plate']

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.copy()
        for pref in self.lab_prefixes_:
            last = f"{pref}_Lab Result of Last Timepoint"
            first = f"{pref}_Lab Result of First Timepoint"
            diff = f"{pref}_Last - First"
            
            if first in X.columns and diff in X.columns and last in X.columns:
                mask_last = X[last].isna() & X[first].notna() & X[diff].notna()
                X.loc[mask_last, last] = X.loc[mask_last, first] + X.loc[mask_last, diff]
                
        return X

FEATURES = [
    "Sepsis",
    "Sex",
    "mv",
    "rrt",
    "ICU_admission",
    "HA_MRSA",
    "chf",
    "cevd",
    "cpd",
    "diab",
    "canc",
    "metacanc",
    "delta SOFA score",
    "prehosp_SOFA_total",
    "hospital_SOFA_total",
    "CCI",
    "Age",
    "df_bili_Lab Result of Last Timepoint",
    "df_bili_Lab Result of First Timepoint",
    "df_crea_Lab Result of Last Timepoint",
    "df_crea_Lab Result of First Timepoint",
    "df_plate_Lab Result of Last Timepoint",
    "df_plate_Lab Result of First Timepoint",
    "df_plate_Last - First",
    "df_crea_Last - First",
    "df_bili_Last - First",
    "MRSA_Lower respiratory",
    "MRSA_Prosthesis/Lines",
    "MRSA_Skin/Wound",
    "MRSA_Urinary",
    "All_Lower respiratory",
    "All_Skin/Wound",
    "All_Systemic",
    "All_Urinary"
]

IMPUTE_MEDIAN = [
    'df_bili_Lab Result of First Timepoint', 'df_crea_Lab Result of First Timepoint',
    'df_plate_Lab Result of First Timepoint',
    'df_bili_Last - First', 'df_crea_Last - First', 'df_plate_Last - First'
]

CONTINUOUS_FEATURES = [
    'Age', 'CCI', 'delta SOFA score', 'prehosp_SOFA_total', 'hospital_SOFA_total',
    'df_bili_Lab Result of Last Timepoint', 'df_bili_Lab Result of First Timepoint',
    'df_crea_Lab Result of Last Timepoint', 'df_crea_Lab Result of First Timepoint',
    'df_plate_Lab Result of Last Timepoint', 'df_plate_Lab Result of First Timepoint',
    'df_plate_Last - First', 'df_crea_Last - First', 'df_bili_Last - First'
]

def build_pipeline(params, dataset_columns):
    target_median = [f for f in IMPUTE_MEDIAN if f in dataset_columns]
    
    selective_imputer = ColumnTransformer(
        transformers=[
            ('med_impute', SimpleImputer(strategy='median'), target_median)
        ],
        remainder='passthrough', verbose_feature_names_out=False
    ).set_output(transform="pandas")

    num_cols = [c for c in CONTINUOUS_FEATURES if c in FEATURES]
    cat_cols = [c for c in FEATURES if c not in num_cols]
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), num_cols), 
            ('cat', 'passthrough', cat_cols)
        ],
        verbose_feature_names_out=False
    )

    return ImbPipeline([
        ('selective_impute', selective_imputer),
        ('post_process', PostProcessingImputer()),
        ('final_prep', preprocessor),
        ('mlp', MLPClassifier(**params))
    ])

def objective(trial):
    n_layers = trial.suggest_int("n_layers", 1, 2)
    layers = tuple(trial.suggest_categorical(f"l{i}", [64, 190]) for i in range(n_layers))
    params = {
        "hidden_layer_sizes": layers, 
        "activation": trial.suggest_categorical("act", ["relu", "tanh"]),
        "alpha": trial.suggest_float("alpha", 1e-4, 1e-1, log=True),
        "learning_rate_init": trial.suggest_float("lr", 1e-4, 1e-2, log=True),
        "random_state": SEED, 
        "max_iter": 1000, 
        "early_stopping": True
    }
    pipeline = build_pipeline(params, list(X_train.columns))
    
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)
    return cross_val_score(pipeline, X_train, y_train, cv=skf, scoring='neg_log_loss', n_jobs=-1).mean()

if __name__ == "__main__":
    logger.info("Loading Data...")
    train_df = pd.read_csv(DATA_PATH/'train.csv')
    test_df = pd.read_csv(DATA_PATH/'test.csv')
    
    available_features = [c for c in FEATURES if c in train_df.columns]
    
    if len(available_features) != len(FEATURES):
        logger.warning(f"Warning: Expected {len(FEATURES)} features, but found {len(available_features)} in dataframe.")

    X_train = train_df[available_features].copy()
    y_train = train_df['Mortality_90day'].values.ravel()
    
    X_test = test_df[available_features].copy()
    y_test = test_df['Mortality_90day'].values.ravel()

    logger.info(f"Starting optimization (Targeting {len(X_train.columns)} features for training)...")
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=N_TRIALS)

    bp = study.best_params
    best_layers = tuple(bp[f"l{i}"] for i in range(bp["n_layers"]))
    final_params = {
        "hidden_layer_sizes": best_layers, 
        "activation": bp["act"], 
        "alpha": bp["alpha"],
        "learning_rate_init": bp["lr"], 
        "random_state": SEED, 
        "max_iter": 2000, 
        "early_stopping": True
    }
    
    final_pipeline = build_pipeline(final_params, list(X_train.columns))
    final_pipeline.fit(X_train, y_train)

    dump(final_pipeline, OUTPUT_DIR / MODEL_FILENAME)
    logger.info(f"Model saved to {OUTPUT_DIR / MODEL_FILENAME}")
